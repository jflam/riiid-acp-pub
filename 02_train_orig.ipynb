{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run distributed:\n",
    "\n",
    "```\n",
    "make\n",
    "```\n",
    "\n",
    "```\n",
    "CUDA_VISIBLE_DEVICES=3,4,5,1,2,0 python -m torch.distributed.launch --master_port 1235 --nproc_per_node=6 02_train.py --epochs 30 --bs 96 --fp16 to_fp16 --trf_heads 4 --mixup False --chunk_size 500 --trf_dim 512 --loss ce --n_chunks 1 --fit fit_flat_cos --fit_kwargs pct_start=0.5 div_final=100 --tfixup True --pad r --valid_pct 0.025 --trf_act gelu --opt ranger_lamb --lr 3e-3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.basics       import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.distributed  import *\n",
    "from fastai.tabular.all  import *\n",
    "from fastai.test_utils   import *\n",
    "\n",
    "import ast\n",
    "import enum\n",
    "import gc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import enum\n",
    "\n",
    "from collections import defaultdict\n",
    "from fastcore.script import *\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from pytorch_block_sparse.util import ModelPatcher\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.distributions.beta import Beta\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_d = Path('input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@call_parse\n",
    "def main(\n",
    "    model:         Param(\"Name\", str) = '210105',\n",
    "    data:          Param(\"Data version\", str) = '210101b',\n",
    "    load:          Param(\"Load from\", str) = None,\n",
    "    validate:      Param(\"\", action='store_true') = False,\n",
    "    chunk_size:    Param(\"Chunk size\", int) = 500,\n",
    "    n_chunks:      Param(\"Number of chunks\", int) = 1,\n",
    "    bs:            Param(\"BS\", int) = 96,\n",
    "    workers:       Param(\"\", int) = 8,\n",
    "    valid_pct:     Param(\"Validation set\", float) = 0.025,\n",
    "    trf_dim:       Param(\"\", int) = 512,\n",
    "    trf_enc:       Param(\"\", int) = 4,\n",
    "    trf_dec:       Param(\"\", int) = 4,\n",
    "    trf_heads:     Param(\"\", int) = 4,\n",
    "    trf_do:        Param(\"\", float) = 0.1,\n",
    "    trf_act:       Param(\"\", str) = 'gelu',\n",
    "    lr:            Param(\"\", float) = 3e-3,\n",
    "    clip:          Param(\"\", float) = 0.,\n",
    "    \n",
    "    moms:          Param(\"Moms for fit_one_cycle\", float, nargs='+') = (0.95,0.85,0.95),\n",
    "    epochs:        Param(\"Epochs\", int) = 30,\n",
    "    tfixup:        Param(\"Use T-Fixup init\", ast.literal_eval) = True,\n",
    "    mixup:         Param(\"Use mixup\", ast.literal_eval) = False,\n",
    "    opt:           Param(\"Optimizer\", str) = 'ranger_lamb',\n",
    "    opt_kwargs:    Param(\"Optional args for opt, eg. eps=1e-4\", str, nargs='+') = {},\n",
    "    fit:           Param(\"fit or fit_one_cycle\", str) = 'fit_flat_cos',\n",
    "    fit_kwargs:    Param(\"Optional args for fit,eg pct_start=0.1\", str, nargs='+') = ['pct_start=0.5', 'div_final=100.'],\n",
    "    fp16:          Param(\"fp16 method: to_fp16, to_native_fp16, none\", str) = 'to_fp16',\n",
    "    \n",
    "    loss:          Param(\"Loss\", str) = 'ce',\n",
    "    \n",
    "    wua:           Param(\"Weight of user_answer term in the loss\", float) = 0.,\n",
    "    pad:           Param (\"Pad left of right (l|r)\",str,choices=['l','r'])='r',\n",
    "\n",
    "    local_rank:    Param(\"--local_rank\", int) = None,\n",
    "): \n",
    "    if opt_kwargs: opt_kwargs = {s.split('=')[0]:float(s.split('=')[1]) for s in opt_kwargs}\n",
    "    if fit_kwargs: fit_kwargs = {s.split('=')[0]:float(s.split('=')[1]) for s in fit_kwargs}\n",
    "    print(locals())\n",
    "    globals().update({ 'H' : AttrDict(locals())})\n",
    "_H = AttrDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': '210105', 'data': '210101b', 'load': None, 'validate': False, 'chunk_size': 500, 'n_chunks': 1, 'bs': 96, 'workers': 8, 'valid_pct': 0.025, 'trf_dim': 512, 'trf_enc': 4, 'trf_dec': 4, 'trf_heads': 4, 'trf_do': 0.1, 'trf_act': 'gelu', 'lr': 0.003, 'clip': 0.0, 'moms': (0.95, 0.85, 0.95), 'epochs': 30, 'tfixup': True, 'mixup': False, 'opt': 'ranger_lamb', 'opt_kwargs': {}, 'fit': 'fit_flat_cos', 'fit_kwargs': {'pct_start': 0.5, 'div_final': 100.0}, 'fp16': 'to_fp16', 'loss': 'ce', 'wua': 0.0, 'pad': 'r', 'local_rank': None}\n"
     ]
    }
   ],
   "source": [
    "#noexport\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpus = torch.cuda.device_count() if H.local_rank is None else 1\n",
    "if H.local_rank is not None:\n",
    "    torch.cuda.set_device(H.local_rank)\n",
    "    torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
    "    print(f\"DISTRIBUTED: {H.local_rank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read df and meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(in_d / f'meta_v{H.data}.pkl', 'rb') as f:\n",
    "    meta = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "QCols = enum.IntEnum('QCols', meta.qcols, start=0)\n",
    "LCols = enum.IntEnum('LCols', meta.lcols, start=0)\n",
    "Cats  = enum.IntEnum('Cats',  meta.cat_names, start=0)\n",
    "Conts = enum.IntEnum('Conts', meta.cont_names, start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "with open(in_d / f'data_v{H.data}.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data.attempt_num_coo\n",
    "del data.attempts_correct_coo\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lut = meta.icats['answered_correctly'],meta.icats['user_answer']\n",
    "y_d = {}\n",
    "for k, v in data.cat_d.items():\n",
    "    y_d[k] = np.column_stack((lut[0][v[:,Cats.answered_correctly] - 1],lut[1][v[:,Cats.user_answer] - 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chop sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chop_sequence(d):\n",
    "    nv = defaultdict(dict)\n",
    "    for k, v in d.items():\n",
    "        i = 0\n",
    "        while i*H.chunk_size < len(v):\n",
    "            nv[k][i] = v[i*H.chunk_size:(i+1)*H.chunk_size]\n",
    "            i += 1\n",
    "    return nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_d  = chop_sequence(data.cat_d)\n",
    "cont_d = chop_sequence(data.cont_d)\n",
    "tags_d = chop_sequence(data.tags_d)\n",
    "tagw_d = chop_sequence(data.tagw_d)\n",
    "y_d    = chop_sequence(y_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert np.concatenate(list(cat_d.values())).shape[0] == np.concatenate(list(data.cat_d.values())).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 393656 different users\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(data.cat_d)} different users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_keys = sorted(list(cat_d.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last H.valid_pct is valid set\n",
    "train_group_keys = group_keys[:int((1 - H.valid_pct) * len(group_keys))]\n",
    "valid_group_keys = group_keys[int((1 - H.valid_pct) * len(group_keys)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users: train=383814, valid=9842\n"
     ]
    }
   ],
   "source": [
    "print(f'users: train={len(train_group_keys)}, valid={len(valid_group_keys)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dict(d, keys):\n",
    "    return { (u, t): d[u][t] for u in keys for t in d[u].keys() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_cat =  split_dict(cat_d, train_group_keys)\n",
    "train_x_cont = split_dict(cont_d, train_group_keys)\n",
    "train_x_tags = split_dict(tags_d, train_group_keys)\n",
    "train_x_tagw = split_dict(tagw_d, train_group_keys)\n",
    "train_y =      split_dict(y_d, train_group_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x_cat =  split_dict(cat_d, valid_group_keys)\n",
    "valid_x_cont = split_dict(cont_d, valid_group_keys)\n",
    "valid_x_tags = split_dict(tags_d, valid_group_keys)\n",
    "valid_x_tagw = split_dict(tagw_d, valid_group_keys)\n",
    "valid_y =      split_dict(y_d, valid_group_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqs: train=507050, valid=12914\n"
     ]
    }
   ],
   "source": [
    "print(f'seqs: train={len(train_x_cat)}, valid={len(valid_x_cat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionsDataset(Dataset):\n",
    "    def __init__(self, x_cat, x_cont, x_tags, x_tagw, y, minids=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.means = np.expand_dims(meta.means, axis=0) # ready to broadcast\n",
    "        self.stds  = np.expand_dims(meta.stds , axis=0)\n",
    "        \n",
    "        self.n_inp = 5  # number of feature (x) tensors\n",
    "        \n",
    "        self.x_cat = x_cat  # SL, XF (sequence len, feature columns) \n",
    "        self.x_cont = x_cont\n",
    "        self.x_tags = x_tags      \n",
    "        self.x_tagw = x_tagw\n",
    "        self.y = y  # SL, 1\n",
    "        \n",
    "        self.keys = list(self.x_cat.keys()) # list of group keys\n",
    "        \n",
    "        if minids:\n",
    "            self.keys = self.keys[:H.bs*2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys) # H.bs * 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_id, time_slice = self.keys[idx]\n",
    "        win = range(max(0, time_slice - H.n_chunks + 1), time_slice + 1)\n",
    "        x_cat  = np.concatenate([ self.x_cat [(user_id, ts)] for ts in win ])\n",
    "        x_cont = np.concatenate([ self.x_cont[(user_id, ts)] for ts in win ])\n",
    "        x_tags = np.concatenate([ self.x_tags[(user_id, ts)] for ts in win ])\n",
    "        x_tagw = np.concatenate([ self.x_tagw[(user_id, ts)] for ts in win ])\n",
    "        y      = np.concatenate([ self.y     [(user_id, ts)] for ts in win ])\n",
    "        \n",
    "        pad = H.chunk_size * H.n_chunks - x_cat.shape[0]\n",
    "        \n",
    "        # Normalize x_cont\n",
    "        x_cont = (x_cont - self.means) / self.stds\n",
    "        x_cont[np.isnan(x_cont)] = 0\n",
    "        \n",
    "        padt = (0,pad) if H.pad == 'r' else (pad,0)\n",
    "        \n",
    "        x_mask = np.zeros(x_cat.shape[0], dtype=np.bool)\n",
    "        \n",
    "        x_mask = np.pad(x_mask, padt, constant_values=(True))\n",
    "        x_cat  = np.pad(x_cat , (padt, (0, 0)), constant_values=(0)).astype(np.int64)\n",
    "        x_cont = np.pad(x_cont, (padt, (0, 0)), constant_values=(0)).astype(np.float32)\n",
    "        x_tags = np.pad(x_tags, (padt, (0, 0)), constant_values=(0)).astype(np.int64)\n",
    "        x_tagw = np.pad(x_tagw, (padt, (0, 0)), constant_values=(0.)).astype(np.float32)\n",
    "        y      = np.pad(y,      (padt, (0, 0)), constant_values=(-1)).astype(np.int64)\n",
    "\n",
    "        return x_mask, x_cat, x_cont, x_tags, x_tagw, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = InteractionsDataset(train_x_cat, train_x_cont, train_x_tags, train_x_tagw, train_y)\n",
    "valid_ds = InteractionsDataset(valid_x_cat, valid_x_cont, valid_x_tags, valid_x_tagw, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mask, x_cat, x_cont, x_tags, x_tagw, y = train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507050"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_tagw[-47:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x_cat.shape == (H.chunk_size*H.n_chunks, len(meta.cat_names))\n",
    "assert x_cont.shape == (H.chunk_size*H.n_chunks, len(meta.cont_names))\n",
    "assert x_tags.shape == x_tagw.shape == (H.chunk_size*H.n_chunks, 6)\n",
    "assert y.shape == (H.chunk_size*H.n_chunks, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs=H.bs, shuffle=True, drop_last=True, num_workers=H.workers)\n",
    "valid_dl = DataLoader(valid_ds, bs=H.bs,                               num_workers=H.workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mask,x_cat, x_cont, x_tags, x_tagw, y = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True)\n",
    "torch.set_printoptions(precision=4, linewidth=200, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2928, -0.3581, -0.1834,  ..., -0.0769,  0.0000,  0.0000],\n",
       "        [-0.2928, -0.3581, -0.1834,  ..., -0.0765, -0.0427, -0.5474],\n",
       "        [-0.2928, -0.3581, -0.1834,  ..., -0.0761, -0.0427, -0.5469],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cont[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([96, 500, 11]),\n",
       " torch.Size([96, 500, 23]),\n",
       " torch.Size([96, 500, 6]),\n",
       " torch.Size([96, 500, 6]),\n",
       " torch.Size([96, 500, 2]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cat.shape, x_cont.shape, x_tags.shape, x_tagw.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x_cat.isnan().any() == False\n",
    "assert x_cont.isnan().any() == False\n",
    "assert x_tags.isnan().any() == False\n",
    "assert x_tagw.isnan().any() == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x_cat.shape == (H.bs, H.chunk_size*H.n_chunks, len(meta.cat_names))\n",
    "assert x_cont.shape == (H.bs, H.chunk_size*H.n_chunks, len(meta.cont_names))\n",
    "assert x_tags.shape == x_tagw.shape == (H.bs, H.chunk_size*H.n_chunks, 6)\n",
    "assert y.shape == (H.bs, H.chunk_size*H.n_chunks, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(pred, targ):\n",
    "    pred = torch.softmax(pred, dim=2)\n",
    "    pred = pred[:,:,1:2] # prediction for True\n",
    "    idx = targ != -1\n",
    "    pred = pred[idx]\n",
    "    targ = targ[idx]\n",
    "    pred, targ = flatten_check(pred, targ)\n",
    "    if len(targ.unique()) == 2:\n",
    "        return roc_auc_score(targ.cpu().numpy(), pred.cpu().numpy())\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss if H.loss=='ce' else globals()[H.loss]\n",
    "loss    = loss_fn(ignore_index=-1)\n",
    "loss_nr = loss_fn(ignore_index=-1, reduction='none')\n",
    "\n",
    "def loss_func(pred, targ, shuffle=None, lam=None):\n",
    "    b, s, l = pred.shape\n",
    "    if shuffle is not None:\n",
    "        targ_shuffled = targ[shuffle].view(b*s)\n",
    "    pred = pred.view(b*s, l)\n",
    "    targ = targ.view(b*s)\n",
    "\n",
    "    if shuffle is not None:\n",
    "        l0 = loss_nr(pred, targ).view(b, s)\n",
    "        l1 = loss_nr(pred, targ_shuffled).view(b, s)\n",
    "        return torch.lerp(l0, l1, lam.view(lam.shape[0], 1)).mean()\n",
    "    else:\n",
    "        #print(targ.unique()) # CUDA assert error if any index here is bigger than dimension l (labels) of pred\n",
    "        return loss(pred, targ)\n",
    "    \n",
    "def ua_loss_func(pred, targ, shuffle=None, lam=None):\n",
    "    loss_fn = loss_func\n",
    "    l = loss_fn(pred[...,:2],targ[...,:1],shuffle,lam) \n",
    "    if H.wua and targ.shape[-1]>1: l += H.wua * loss_fn(pred[...,2:],targ[...,1:],shuffle,lam)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "_p = torch.zeros([32, 127, 6])\n",
    "_t = torch.empty ([32, 127,2]).type(torch.long)\n",
    "_t[...,0] = torch.randint(2,_t.shape[:2])\n",
    "_t[...,1] = torch.randint(4,_t.shape[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#noexport\n",
    "roc_auc(_p[...,:2], _t[...,:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6931)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#noexport\n",
    "loss_func(_p[...,:2], _t[...,:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6931)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#noexport\n",
    "ua_loss_func(_p, _t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LBMetric(Metric):\n",
    "    def __init__(self, loss_func, name):\n",
    "        self.loss_func = loss_func\n",
    "        self.nam = name\n",
    "        \n",
    "    def reset(self):\n",
    "        self.targs, self.preds = [], []\n",
    "        \n",
    "    def accumulate(self, learn):\n",
    "        self.preds.append(learn.to_detach(learn.pred[...,:2]))\n",
    "        self.targs.append(learn.to_detach(learn.y[...,:1]))\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        if len(self.preds) == 0: return\n",
    "        preds = torch.cat(self.preds)\n",
    "        targs = torch.cat(self.targs)\n",
    "        r = self.loss_func(preds, targs)\n",
    "        return r\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.nam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#noexport\n",
    "lam = Beta(0.5, 0.5).sample((10000,))\n",
    "lam = torch.stack([lam, 1-lam], 1)\n",
    "lam = lam.max(1)[0].numpy()\n",
    "_ = plt.hist(lam, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMixUp(Callback):\n",
    "    run_after,run_valid = [Normalize],False\n",
    "    def __init__(self, alpha=0.4): \n",
    "        self.distrib = Beta(tensor(alpha), tensor(alpha))\n",
    "\n",
    "    def before_batch(self):\n",
    "        lam = self.distrib.sample((self.y.size(0),)).squeeze().to(self.y.device)\n",
    "        lam = torch.stack([lam, 1-lam], 1)\n",
    "        lam = lam.max(1)[0]\n",
    "        shuffle = torch.randperm(self.y.size(0)).to(self.y.device)\n",
    "        self.learn.xb = (*self.xb, shuffle, lam)\n",
    "        self.learn.yb = (*self.yb, shuffle, lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientClipping(Callback):\n",
    "    \"Gradient clipping during training.\"\n",
    "    def __init__(self, clip:float = 0.):\n",
    "        self.clip = clip\n",
    "\n",
    "    def after_backward(self, **kwargs):\n",
    "        \"Clip the gradient before the optimizer step.\"\n",
    "        if self.clip: nn.utils.clip_grad_norm_(self.learn.model.parameters(), self.clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TutorNet(nn.Module):\n",
    "    def __init__(self, emb_szs, tag_emb_szs, emb_do, n_cont, trf_dim, trf_enc, trf_dec, trf_heads, trf_do, trf_act):\n",
    "        super().__init__()\n",
    "        self.nhead,self.trf_dim = trf_heads, trf_dim\n",
    "        \n",
    "        tag_emb_szs =(tag_emb_szs[0]+1, trf_dim)\n",
    "\n",
    "        self.embeds    = nn.ModuleList([nn.Sequential(nn.Embedding(ni+1, nf, max_norm=1.),nn.Linear(nf, trf_dim)) \n",
    "                                        for ni, nf in emb_szs])\n",
    "        self.tagembeds = nn.EmbeddingBag(*tag_emb_szs, max_norm=1., mode='sum')\n",
    "        self.conts     = nn.Linear(n_cont, trf_dim)\n",
    "            \n",
    "        self.trafo = nn.Transformer(\n",
    "            d_model = trf_dim,\n",
    "            nhead = trf_heads,\n",
    "            num_encoder_layers = trf_enc,\n",
    "            num_decoder_layers = trf_dec,\n",
    "            dim_feedforward = trf_dim*4,\n",
    "            dropout = trf_do,\n",
    "            activation = trf_act,\n",
    "        )\n",
    "\n",
    "        self.mlp = nn.Linear(trf_dim, 6)\n",
    "        \n",
    "    def forward(self, x_mask, x_cat, x_cont, x_tags, x_tagw, shuffle=None, lam=None):\n",
    "        b, sl, catf, contf, tagsf = (*x_cat.shape, x_cont.shape[2], x_tags.shape[2])\n",
    "        \n",
    "        x_cat  += 1\n",
    "        x_tags += 1\n",
    "    \n",
    "        # compute masks\n",
    "        causal_mask  = torch.triu(torch.ones(1,sl, sl,dtype=torch.bool,device=x_cat.device), diagonal=1).expand(b,-1,-1)\n",
    "        x_tci   = x_cat[...,Cats.task_container_id]\n",
    "        x_tci_s = torch.zeros_like(x_tci)\n",
    "        x_tci_s[...,1:] = x_tci[...,:-1]\n",
    "        enc_container_aware_mask =  (x_tci.unsqueeze(-1) == x_tci_s.unsqueeze(-1).permute(0,2,1)) | causal_mask\n",
    "        dec_container_aware_mask = ~(x_tci.unsqueeze(-1) == x_tci.unsqueeze(-1).permute(0,2,1))   & causal_mask\n",
    "\n",
    "        padding_mask = x_mask \n",
    "                \n",
    "        # encoder x (shifted q & a)\n",
    "        enc_cat  = torch.zeros_like(x_cat)\n",
    "        enc_cont = torch.zeros_like(x_cont)\n",
    "        enc_tags = torch.zeros_like(x_tags)\n",
    "        enc_tagw = torch.zeros_like(x_tagw)\n",
    "        \n",
    "        enc_cat[:,1:]  = x_cat[:,:-1]\n",
    "        enc_cont[:,1:] = x_cont[:,:-1]\n",
    "        enc_tags[:,1:] = x_tags[:,:-1]\n",
    "        enc_tagw[:,1:] = x_tagw[:,:-1]\n",
    "        \n",
    "        # decoder x (nonshifted q)\n",
    "        dec_cat  = x_cat\n",
    "        dec_cont = x_cont\n",
    "        dec_tags = x_tags\n",
    "        dec_tagw = x_tagw\n",
    "\n",
    "        # hide correct answer and user answered correctly from decoder\n",
    "        dec_cat[...,Cats.answered_correctly] = 0\n",
    "        dec_cat[...,Cats.user_answer] = 0\n",
    "        dec_cat[...,Cats.qhe] = 0\n",
    "        dec_cont[...,Conts.qet] = 0\n",
    "        dec_cont[...,Conts.qet_log] = 0\n",
    "        \n",
    "        # print(enc_cont.shape)\n",
    "        enc_cat  =  enc_cat.view(b * sl, catf)   # b*sl, catf\n",
    "        enc_tags = enc_tags.view(b * sl, tagsf) # b*sl, tagsf\n",
    "        enc_tagw = enc_tagw.view(b * sl, tagsf) # b*sl, tagsf\n",
    "\n",
    "        dec_cat  =  dec_cat.view(b * sl, catf)   # b*sl, catf\n",
    "        dec_tags = dec_tags.view(b * sl, tagsf) # b*sl, tagsf\n",
    "        dec_tagw = dec_tagw.view(b * sl, tagsf) # b*sl, tagsf\n",
    "        \n",
    "        # embed categorical vars\n",
    "        enc = torch.mean(torch.stack([\n",
    "            *[ e(enc_cat[:,i]) for i, e in enumerate(self.embeds) ],\n",
    "            self.tagembeds(enc_tags, per_sample_weights=enc_tagw),\n",
    "            self.conts(enc_cont).view(-1,self.trf_dim)\n",
    "        ]),dim=0)\n",
    "        \n",
    "        dec = torch.mean(torch.stack([\n",
    "            *[ e(dec_cat[:,i]) for i, e in enumerate(self.embeds) ],\n",
    "            self.tagembeds(dec_tags, per_sample_weights=dec_tagw),\n",
    "            self.conts(dec_cont).view(-1,self.trf_dim)\n",
    "        ]),dim=0)\n",
    "        \n",
    "        enc = enc.view(b, sl, self.trf_dim)           # b, sl, sum of cat, cont and tag ftrs\n",
    "        dec = dec.view(b, sl, self.trf_dim)           # b, sl, sum of cat, cont and tag ftrs\n",
    "\n",
    "        if shuffle is not None:\n",
    "            enc = torch.lerp(enc, enc[shuffle], lam.view(lam.shape[0], 1, 1))\n",
    "            dec = torch.lerp(dec, dec[shuffle], lam.view(lam.shape[0], 1, 1))\n",
    "            padding_mask = None\n",
    "            enc_container_aware_mask = dec_container_aware_mask = causal_mask | causal_mask[shuffle]\n",
    "        \n",
    "        enc = enc.permute(1, 0, 2)          # sl, b, tf (torchformer input)\n",
    "        dec = dec.permute(1, 0, 2)          # sl, b, tf\n",
    "\n",
    "        expand_nheads = lambda t: t.unsqueeze(1).expand(t.shape[0],self.nhead,-1,-1).reshape(-1,*t.shape[-2:])\n",
    "        \n",
    "        o = self.trafo(\n",
    "            enc, \n",
    "            dec, \n",
    "            src_mask = expand_nheads(enc_container_aware_mask),\n",
    "            tgt_mask = expand_nheads(dec_container_aware_mask),\n",
    "            memory_mask = expand_nheads(enc_container_aware_mask),\n",
    "            src_key_padding_mask = padding_mask,\n",
    "            tgt_key_padding_mask = padding_mask,\n",
    "            memory_key_padding_mask = padding_mask,\n",
    "        )                                   # sl, b, tf\n",
    "        o = o.permute(1, 0, 2)              # b, sl, tf\n",
    "        o = self.mlp(o)                     # b, sl, of (of=2)\n",
    "        #print(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_szs = list(zip(meta.n_emb.values(), meta.emb_dim.values()))\n",
    "tag_emb_szs = meta.tags_n_emb, meta.tags_emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TutorNet(emb_szs, tag_emb_szs, None, len(meta.cont_names), \n",
    "                 H.trf_dim, H.trf_enc, H.trf_dec, H.trf_heads, H.trf_do, H.trf_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-Fixup init\n",
    "\n",
    "1. Apply Xavier initialization for all parameters excluding input embeddings. Use Gaussian initialization $N(0,d^{-\\frac{1}{2}})$ for input embeddings where d is the embedding dimension.\n",
    "\n",
    "2. Scale $v_{d}$ and $w_{d}$ matrices in each decoder attention block, weight matrices in each decoder MLP block and input embeddings $x$ and $y$ in encoder and decoder by $(9N)^{−\\frac{1}{4}}$: [code](https://github.com/layer6ai-labs/T-Fixup/blob/f1fae213ce7b48829f81632d0c96bb039b7c450e/fairseq/modules/transformer_layer.py#L161), [code](https://github.com/layer6ai-labs/T-Fixup/blob/f1fae213ce7b48829f81632d0c96bb039b7c450e/fairseq/models/transformer.py#L378), [code](https://github.com/layer6ai-labs/T-Fixup/blob/f1fae213ce7b48829f81632d0c96bb039b7c450e/fairseq/models/transformer.py#L604)\n",
    "\n",
    "3. Scale $v_{e}$ and $w_{e}$ matrices in each encoder attention block and weight matrices in each encoder MLP block by $0.67N^{−\\frac{1}{4}}$: [code](https://github.com/layer6ai-labs/T-Fixup/blob/f1fae213ce7b48829f81632d0c96bb039b7c450e/fairseq/modules/transformer_layer.py#L36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trunc_normal_(x, mean=0., std=1.):\n",
    "    \"Truncated normal initialization (approximation)\"\n",
    "    # From https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/12\n",
    "    return x.normal_().fmod_(2).mul_(std).add_(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if H.tfixup:\n",
    "    for n,p in model.named_parameters():\n",
    "        if re.match(r'.*bias$|.*bn\\.weight$|.*norm.*\\.weight',n): continue\n",
    "        gain = 1.\n",
    "        if re.match(r'.*decoder.*',n): \n",
    "            gain = (9*H.trf_dec)**(-1./4.)\n",
    "            if re.match(f'.*in_proj_weight$',n): gain *= (2**0.5)\n",
    "        elif re.match(r'.*encoder.*',n): \n",
    "            gain = 0.67*(H.trf_enc**(-1./4.))\n",
    "            if re.match(f'.*in_proj_weight$',n): gain *= (2**0.5)\n",
    "        if re.match(r'^embeds|^tagembeds', n): \n",
    "            trunc_normal_(p.data,std=(4.5*(H.trf_enc+H.trf_dec))**(-1./4.)*H.trf_dim**(-0.5))\n",
    "        else:                                  \n",
    "            nn.init.xavier_normal_(p,gain=gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "if H.tfixup:\n",
    "    class MyModelPatcher(ModelPatcher):\n",
    "        def new_child_module(self, child_module_name, child_module, patch_info): return nn.Identity()\n",
    "    mp = MyModelPatcher()\n",
    "    mp.add_pattern(r\".*norm\\d?.*\",{})\n",
    "    mp.patch_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TutorNet(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Embedding(3, 1, max_norm=1.0)\n",
       "      (1): Linear(in_features=1, out_features=512, bias=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Embedding(5, 3, max_norm=1.0)\n",
       "      (1): Linear(in_features=3, out_features=512, bias=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Embedding(9767, 274, max_norm=1.0)\n",
       "      (1): Linear(in_features=274, out_features=512, bias=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Embedding(6, 4, max_norm=1.0)\n",
       "      (1): Linear(in_features=4, out_features=512, bias=True)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Embedding(420, 47, max_norm=1.0)\n",
       "      (1): Linear(in_features=47, out_features=512, bias=True)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Embedding(9, 5, max_norm=1.0)\n",
       "      (1): Linear(in_features=5, out_features=512, bias=True)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Embedding(4, 3, max_norm=1.0)\n",
       "      (1): Linear(in_features=3, out_features=512, bias=True)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Embedding(13525, 329, max_norm=1.0)\n",
       "      (1): Linear(in_features=329, out_features=512, bias=True)\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Embedding(10002, 278, max_norm=1.0)\n",
       "      (1): Linear(in_features=278, out_features=512, bias=True)\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Embedding(6, 4, max_norm=1.0)\n",
       "      (1): Linear(in_features=4, out_features=512, bias=True)\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): Embedding(7, 4, max_norm=1.0)\n",
       "      (1): Linear(in_features=4, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (tagembeds): EmbeddingBag(190, 512, max_norm=1.0, mode=sum)\n",
       "  (conts): Linear(in_features=23, out_features=512, bias=True)\n",
       "  (trafo): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "          (norm3): Identity()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "          (norm3): Identity()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "          (norm3): Identity()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "          (norm3): Identity()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): Identity()\n",
       "    )\n",
       "  )\n",
       "  (mlp): Linear(in_features=512, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#noexport\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dls.cuda()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delegates(Lamb)\n",
    "def ranger_lamb(p, lr, mom=0.95, wd=0.01, eps=1e-6, **kwargs):\n",
    "    return Lookahead(Lamb(p, lr=lr, mom=mom, wd=wd, eps=eps, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(\n",
    "    dls,\n",
    "    model,\n",
    "    loss_func=ua_loss_func,\n",
    "    opt_func=partial(globals()[H.opt],**H.opt_kwargs),\n",
    "    moms = H.moms,\n",
    "    metrics=[\n",
    "        LBMetric(loss_func, 'acc_valid_loss'),\n",
    "        LBMetric(roc_auc, 'acc_roc_auc'),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_fp16 = getattr(learn,H.fp16,None)\n",
    "if f_fp16: f_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank0_only(func, *args, **kwargs):\n",
    "    \"Execute `func` in the Rank-0 process first, then in other ranks in parallel.\"\n",
    "    if args or kwargs: func = partial(func, *args, **kwargs)\n",
    "    dummy_l = Learner(DataLoaders(device='cpu'), nn.Linear(1,1), loss_func=lambda: 0)\n",
    "    res = None\n",
    "    with dummy_l.distrib_ctx():\n",
    "        if not rank_distrib(): res = func()\n",
    "        distrib_barrier()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def load(learn:Learner,fn,with_opt=False):\n",
    "    def __inner(learn:Learner,fn,with_opt=False):\n",
    "        m_dict = torch.load(f\"{(Path(learn.model_dir) / fn)}.pth\")#['model']\n",
    "        ks = []\n",
    "        for attempts in range(2):\n",
    "            try:\n",
    "                res = learn.model.load_state_dict(m_dict,strict=False)\n",
    "                print(f\"Loaded {fn} ignoring: {' '.join(ks)} and {res}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                for k in [m[1] for m in [re.match(r\"^.*mismatch for ([\\w\\.]+):\",l) for l in str(e).split(\"\\n\")] if m is not None]:\n",
    "                    m_dict.pop(k,None)\n",
    "                    ks.append(k)\n",
    "        return learn\n",
    "    return rank0_only(__inner, learn, fn, with_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if H.load:\n",
    "    learn.load(H.load, with_opt=False)\n",
    "    print(f\"Loaded: {H.load}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if H.clip: \n",
    "    learn.add_cb(GradientClipping(H.clip))\n",
    "    print('clip on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "if H.local_rank is not None: \n",
    "    learn.to_distributed(H.local_rank)\n",
    "    print('local_rank on')\n",
    "if H.mixup: \n",
    "    learn.add_cb(MyMixUp(0.5))\n",
    "    print('mixup on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "if H.validate:\n",
    "    res = learn.validate()\n",
    "    print(f\"CV: {res[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TutorNet (Input shape: 96 x 500)\n",
       "============================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "============================================================================\n",
       "                     96 x 1              \n",
       "Embedding                                 3          True      \n",
       "____________________________________________________________________________\n",
       "                     96 x 512            \n",
       "Linear                                    1024       True      \n",
       "____________________________________________________________________________\n",
       "                     96 x 3              \n",
       "Embedding                                 15         True      \n",
       "____________________________________________________________________________\n",
       "                     96 x 512            \n",
       "Linear                                    2048       True      \n",
       "____________________________________________________________________________\n",
       "                     96 x 274            \n",
       "Embedding                                 2676158    True      \n",
       "____________________________________________________________________________\n",
       "                     96 x 512            \n",
       "Linear                                    140800     True      \n",
       "____________________________________________________________________________\n",
       "                     96 x 4              \n",
       "Embedding                                 24         True      \n",
       "____________________________________________________________________________\n",
       "                     96 x 512            \n",
       "Linear                                    2560       True      \n",
       "____________________________________________________________________________\n",
       "                     96 x 47             \n",
       "Embedding                                 19740      True      \n",
       "____________________________________________________________________________\n",
       "                     96 x 512            \n",
       "Linear                                    24576      True      \n",
       "____________________________________________________________________________\n",
       "                     96 x 5              \n",
       "Embedding                                 45         True      \n",
       "____________________________________________________________________________\n",
       "                     96 x 512            \n",
       "Linear                                    3072       True      \n",
       "____________________________________________________________________________\n",
       "                     96 x 3              \n",
       "Embedding                                 12         True      \n",
       "____________________________________________________________________________\n",
       "                     96 x 512            \n",
       "Linear                                    2048       True      \n",
       "____________________________________________________________________________\n",
       "                     96 x 329            \n",
       "Embedding                                 4449725    True      \n",
       "____________________________________________________________________________\n",
       "                     96 x 512            \n",
       "Linear                                    168960     True      \n",
       "____________________________________________________________________________\n",
       "                     96 x 278            \n",
       "Embedding                                 2780556    True      \n",
       "____________________________________________________________________________\n",
       "                     96 x 512            \n",
       "Linear                                    142848     True      \n",
       "____________________________________________________________________________\n",
       "                     96 x 4              \n",
       "Embedding                                 24         True      \n",
       "____________________________________________________________________________\n",
       "                     96 x 512            \n",
       "Linear                                    2560       True      \n",
       "____________________________________________________________________________\n",
       "                     96 x 4              \n",
       "Embedding                                 28         True      \n",
       "____________________________________________________________________________\n",
       "                     96 x 512            \n",
       "Linear                                    2560       True      \n",
       "EmbeddingBag                              97280      True      \n",
       "____________________________________________________________________________\n",
       "                     96 x 500 x 512      \n",
       "Linear                                    12288      True      \n",
       "____________________________________________________________________________\n",
       "                     96 x 1 x 2048       \n",
       "Linear                                    1050624    True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     96 x 1 x 512        \n",
       "Linear                                    1049088    True      \n",
       "Identity                                                       \n",
       "Identity                                                       \n",
       "Dropout                                                        \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     96 x 1 x 2048       \n",
       "Linear                                    1050624    True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     96 x 1 x 512        \n",
       "Linear                                    1049088    True      \n",
       "Identity                                                       \n",
       "Identity                                                       \n",
       "Dropout                                                        \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     96 x 1 x 2048       \n",
       "Linear                                    1050624    True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     96 x 1 x 512        \n",
       "Linear                                    1049088    True      \n",
       "Identity                                                       \n",
       "Identity                                                       \n",
       "Dropout                                                        \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     96 x 1 x 2048       \n",
       "Linear                                    1050624    True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     96 x 1 x 512        \n",
       "Linear                                    1049088    True      \n",
       "Identity                                                       \n",
       "Identity                                                       \n",
       "Dropout                                                        \n",
       "Dropout                                                        \n",
       "Identity                                                       \n",
       "____________________________________________________________________________\n",
       "                     96 x 1 x 2048       \n",
       "Linear                                    1050624    True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     96 x 1 x 512        \n",
       "Linear                                    1049088    True      \n",
       "Identity                                                       \n",
       "Identity                                                       \n",
       "Identity                                                       \n",
       "Dropout                                                        \n",
       "Dropout                                                        \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     96 x 1 x 2048       \n",
       "Linear                                    1050624    True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     96 x 1 x 512        \n",
       "Linear                                    1049088    True      \n",
       "Identity                                                       \n",
       "Identity                                                       \n",
       "Identity                                                       \n",
       "Dropout                                                        \n",
       "Dropout                                                        \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     96 x 1 x 2048       \n",
       "Linear                                    1050624    True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     96 x 1 x 512        \n",
       "Linear                                    1049088    True      \n",
       "Identity                                                       \n",
       "Identity                                                       \n",
       "Identity                                                       \n",
       "Dropout                                                        \n",
       "Dropout                                                        \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     96 x 1 x 2048       \n",
       "Linear                                    1050624    True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     96 x 1 x 512        \n",
       "Linear                                    1049088    True      \n",
       "Identity                                                       \n",
       "Identity                                                       \n",
       "Identity                                                       \n",
       "Dropout                                                        \n",
       "Dropout                                                        \n",
       "Dropout                                                        \n",
       "Identity                                                       \n",
       "____________________________________________________________________________\n",
       "                     96 x 500 x 6        \n",
       "Linear                                    3078       True      \n",
       "____________________________________________________________________________\n",
       "\n",
       "Total params: 27,329,728\n",
       "Total trainable params: 27,329,728\n",
       "Total non-trainable params: 0\n",
       "\n",
       "Optimizer used: functools.partial(<function ranger_lamb at 0x7f0f98c90430>)\n",
       "Loss function: <function ua_loss_func at 0x7f0fa470e8b0>\n",
       "\n",
       "Callbacks:\n",
       "  - TrainEvalCallback\n",
       "  - MixedPrecision\n",
       "  - Recorder\n",
       "  - ProgressCallback"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#noexport\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "#learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "#learn.recorder.plot_lr_find(skip_end=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#short_cb = learn.add_cb(ShortEpochCallback(pct=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai.learner.Learner at 0x7f0f98c8c8e0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.add_cb(SaveModelCallback(monitor='acc_roc_auc', comp=np.greater, fname=f'best{H.model}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_flat_cos 30 0.003 {'pct_start': 0.5, 'div_final': 100.0}\n"
     ]
    }
   ],
   "source": [
    "print(H.fit, H.epochs, H.lr, H.fit_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>12.601474</td>\n",
       "      <td>8.964695</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.398618</td>\n",
       "      <td>2.164358</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.368629</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.617315</td>\n",
       "      <td>0.052348</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.603572</td>\n",
       "      <td>0.090404</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.947144</td>\n",
       "      <td>0.052820</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.484871</td>\n",
       "      <td>0.020400</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.149363</td>\n",
       "      <td>0.011796</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.900711</td>\n",
       "      <td>0.011633</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.712989</td>\n",
       "      <td>0.011502</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.568821</td>\n",
       "      <td>0.011014</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.456661</td>\n",
       "      <td>0.010861</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.368572</td>\n",
       "      <td>0.010690</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.298823</td>\n",
       "      <td>0.010651</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.243289</td>\n",
       "      <td>0.010669</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.198856</td>\n",
       "      <td>0.010639</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.163147</td>\n",
       "      <td>0.010696</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.134368</td>\n",
       "      <td>0.010720</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.111107</td>\n",
       "      <td>0.010658</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.092287</td>\n",
       "      <td>0.010708</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.077002</td>\n",
       "      <td>0.010716</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.064589</td>\n",
       "      <td>0.010689</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.054495</td>\n",
       "      <td>0.010693</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.046292</td>\n",
       "      <td>0.010680</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.039586</td>\n",
       "      <td>0.010674</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.034110</td>\n",
       "      <td>0.010675</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.029685</td>\n",
       "      <td>0.010683</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.026057</td>\n",
       "      <td>0.010685</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.023107</td>\n",
       "      <td>0.010687</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.020691</td>\n",
       "      <td>0.010687</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#noexport\n",
    "syn_learn = synth_learner()\n",
    "getattr(syn_learn,H.fit)(H.epochs, H.lr,**H.fit_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#noexport\n",
    "syn_learn.recorder.plot_sched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- model: 210105\n",
       "- data: 210101b\n",
       "- load: None\n",
       "- validate: False\n",
       "- chunk_size: 500\n",
       "- n_chunks: 1\n",
       "- bs: 96\n",
       "- workers: 8\n",
       "- valid_pct: 0.025\n",
       "- trf_dim: 512\n",
       "- trf_enc: 4\n",
       "- trf_dec: 4\n",
       "- trf_heads: 4\n",
       "- trf_do: 0.1\n",
       "- trf_act: gelu\n",
       "- lr: 0.003\n",
       "- clip: 0.0\n",
       "- moms: (0.95, 0.85, 0.95)\n",
       "- epochs: 30\n",
       "- tfixup: True\n",
       "- mixup: False\n",
       "- opt: ranger_lamb\n",
       "- opt_kwargs: \n",
       "\n",
       "- fit: fit_flat_cos\n",
       "- fit_kwargs: \n",
       "  - pct_start: 0.5\n",
       "  - div_final: 100.0\n",
       "- fp16: to_fp16\n",
       "- loss: ce\n",
       "- wua: 0.0\n",
       "- pad: r\n",
       "- local_rank: None"
      ],
      "text/plain": [
       "- model: 210105\n",
       "- data: 210101b\n",
       "- load: None\n",
       "- validate: False\n",
       "- chunk_size: 500\n",
       "- n_chunks: 1\n",
       "- bs: 96\n",
       "- workers: 8\n",
       "- valid_pct: 0.025\n",
       "- trf_dim: 512\n",
       "- trf_enc: 4\n",
       "- trf_dec: 4\n",
       "- trf_heads: 4\n",
       "- trf_do: 0.1\n",
       "- trf_act: gelu\n",
       "- lr: 0.003\n",
       "- clip: 0.0\n",
       "- moms: (0.95, 0.85, 0.95)\n",
       "- epochs: 30\n",
       "- tfixup: True\n",
       "- mixup: False\n",
       "- opt: ranger_lamb\n",
       "- opt_kwargs: \n",
       "\n",
       "- fit: fit_flat_cos\n",
       "- fit_kwargs: \n",
       "  - pct_start: 0.5\n",
       "  - div_final: 100.0\n",
       "- fp16: to_fp16\n",
       "- loss: ce\n",
       "- wua: 0.0\n",
       "- pad: r\n",
       "- local_rank: None"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/30 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>acc_valid_loss</th>\n",
       "      <th>acc_roc_auc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='5281' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/5281 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 376.00 MiB (GPU 0; 15.78 GiB total capacity; 13.76 GiB already allocated; 305.50 MiB free; 14.41 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-7d091f940051>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Fitting {H.local_rank}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/fastai/callback/schedule.py\u001b[0m in \u001b[0;36mfit_flat_cos\u001b[0;34m(self, n_epoch, lr, div_final, pct_start, wd, cbs, reset_opt)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mscheds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcombined_cos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpct_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdiv_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mParamScheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_opt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_hypers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelFitException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_end_cleanup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_end_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelEpochException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_opt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelTrainException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mall_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mone_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_one_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelBatchException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_pred'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-bf0ae3080453>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_mask, x_cat, x_cont, x_tags, x_tagw, shuffle, lam)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mexpand_nheads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnhead\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         o = self.trafo(\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n\u001b[0m\u001b[1;32m    125\u001b[0m                               \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                               memory_key_padding_mask=memory_key_padding_mask)\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             output = mod(output, memory, tgt_mask=tgt_mask,\n\u001b[0m\u001b[1;32m    232\u001b[0m                          \u001b[0mmemory_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                          \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0mtgt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mgelu\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m   1381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 376.00 MiB (GPU 0; 15.78 GiB total capacity; 13.76 GiB already allocated; 305.50 MiB free; 14.41 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "print(f\"Fitting {H.local_rank}\")\n",
    "getattr(learn,H.fit)(H.epochs, H.lr,**H.fit_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "learn.recorder.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
