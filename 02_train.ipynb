{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run distributed:\n",
    "\n",
    "```\n",
    "make\n",
    "```\n",
    "\n",
    "```\n",
    "CUDA_VISIBLE_DEVICES=3,1,2,0 python -m torch.distributed.launch --master_port 1235 --nproc_per_node=4 02_train.py --epochs 30 --bs 96 --fp16 to_fp16 --trf_heads 4 --mixup False --chunk_size 500 --trf_dim 512 --loss ce --n_chunks 1 --fit fit_flat_cos --fit_kwargs pct_start=0.5 div_final=100 --tfixup True --pad r --valid_pct 0.025 --trf_act gelu --opt ranger_lamb --lr 3e-3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.basics       import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.distributed  import *\n",
    "from fastai.tabular.all  import *\n",
    "from fastai.test_utils   import *\n",
    "\n",
    "import ast\n",
    "import enum\n",
    "import gc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import enum\n",
    "\n",
    "from collections import defaultdict\n",
    "from fastcore.script import *\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from pytorch_block_sparse.util import ModelPatcher\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.distributions.beta import Beta\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_d = Path('/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@call_parse\n",
    "def main(\n",
    "    model:         Param(\"Name\", str) = '210105',\n",
    "    data:          Param(\"Data version\", str) = '210101b',\n",
    "    load:          Param(\"Load from\", str) = None,\n",
    "    validate:      Param(\"\", action='store_true') = False,\n",
    "    chunk_size:    Param(\"Chunk size\", int) = 500,\n",
    "    n_chunks:      Param(\"Number of chunks\", int) = 1,\n",
    "    bs:            Param(\"BS\", int) = 96,\n",
    "    workers:       Param(\"\", int) = 8,\n",
    "    valid_pct:     Param(\"Validation set\", float) = 0.025,\n",
    "    trf_dim:       Param(\"\", int) = 512,\n",
    "    trf_enc:       Param(\"\", int) = 4,\n",
    "    trf_dec:       Param(\"\", int) = 4,\n",
    "    trf_heads:     Param(\"\", int) = 4,\n",
    "    trf_do:        Param(\"\", float) = 0.1,\n",
    "    trf_act:       Param(\"\", str) = 'gelu',\n",
    "    lr:            Param(\"\", float) = 3e-3,\n",
    "    clip:          Param(\"\", float) = 0.,\n",
    "    \n",
    "    moms:          Param(\"Moms for fit_one_cycle\", float, nargs='+') = (0.95,0.85,0.95),\n",
    "    epochs:        Param(\"Epochs\", int) = 30,\n",
    "    tfixup:        Param(\"Use T-Fixup init\", ast.literal_eval) = True,\n",
    "    mixup:         Param(\"Use mixup\", ast.literal_eval) = False,\n",
    "    opt:           Param(\"Optimizer\", str) = 'ranger_lamb',\n",
    "    opt_kwargs:    Param(\"Optional args for opt, eg. eps=1e-4\", str, nargs='+') = {},\n",
    "    fit:           Param(\"fit or fit_one_cycle\", str) = 'fit_flat_cos',\n",
    "    fit_kwargs:    Param(\"Optional args for fit,eg pct_start=0.1\", str, nargs='+') = ['pct_start=0.5', 'div_final=100.'],\n",
    "    fp16:          Param(\"fp16 method: to_fp16, to_native_fp16, none\", str) = 'to_fp16',\n",
    "    \n",
    "    loss:          Param(\"Loss\", str) = 'ce',\n",
    "    \n",
    "    wua:           Param(\"Weight of user_answer term in the loss\", float) = 0.,\n",
    "    pad:           Param (\"Pad left of right (l|r)\",str,choices=['l','r'])='r',\n",
    "\n",
    "    local_rank:    Param(\"--local_rank\", int) = None,\n",
    "): \n",
    "    if opt_kwargs: opt_kwargs = {s.split('=')[0]:float(s.split('=')[1]) for s in opt_kwargs}\n",
    "    if fit_kwargs: fit_kwargs = {s.split('=')[0]:float(s.split('=')[1]) for s in fit_kwargs}\n",
    "    print(locals())\n",
    "    globals().update({ 'H' : AttrDict(locals())})\n",
    "_H = AttrDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': '210105', 'data': '210101b', 'load': None, 'validate': False, 'chunk_size': 500, 'n_chunks': 1, 'bs': 96, 'workers': 8, 'valid_pct': 0.025, 'trf_dim': 512, 'trf_enc': 4, 'trf_dec': 4, 'trf_heads': 4, 'trf_do': 0.1, 'trf_act': 'gelu', 'lr': 0.003, 'clip': 0.0, 'moms': (0.95, 0.85, 0.95), 'epochs': 30, 'tfixup': True, 'mixup': False, 'opt': 'ranger_lamb', 'opt_kwargs': {}, 'fit': 'fit_flat_cos', 'fit_kwargs': {'pct_start': 0.5, 'div_final': 100.0}, 'fp16': 'to_fp16', 'loss': 'ce', 'wua': 0.0, 'pad': 'r', 'local_rank': None}\n"
     ]
    }
   ],
   "source": [
    "#noexport\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpus = torch.cuda.device_count() if H.local_rank is None else 1\n",
    "if H.local_rank is not None:\n",
    "    torch.cuda.set_device(H.local_rank)\n",
    "    torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
    "    print(f\"DISTRIBUTED: {H.local_rank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read df and meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(in_d / f'meta_v{H.data}.pkl', 'rb') as f:\n",
    "    meta = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "QCols = enum.IntEnum('QCols', meta.qcols, start=0)\n",
    "LCols = enum.IntEnum('LCols', meta.lcols, start=0)\n",
    "Cats  = enum.IntEnum('Cats',  meta.cat_names, start=0)\n",
    "Conts = enum.IntEnum('Conts', meta.cont_names, start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "with open(in_d / f'data_v{H.data}.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data.attempt_num_coo\n",
    "del data.attempts_correct_coo\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lut = meta.icats['answered_correctly'],meta.icats['user_answer']\n",
    "y_d = {}\n",
    "for k, v in data.cat_d.items():\n",
    "    y_d[k] = np.column_stack((lut[0][v[:,Cats.answered_correctly] - 1],lut[1][v[:,Cats.user_answer] - 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chop sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chop_sequence(d):\n",
    "    nv = defaultdict(dict)\n",
    "    for k, v in d.items():\n",
    "        i = 0\n",
    "        while i*H.chunk_size < len(v):\n",
    "            nv[k][i] = v[i*H.chunk_size:(i+1)*H.chunk_size]\n",
    "            i += 1\n",
    "    return nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_d  = chop_sequence(data.cat_d)\n",
    "cont_d = chop_sequence(data.cont_d)\n",
    "tags_d = chop_sequence(data.tags_d)\n",
    "tagw_d = chop_sequence(data.tagw_d)\n",
    "y_d    = chop_sequence(y_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert np.concatenate(list(cat_d.values())).shape[0] == np.concatenate(list(data.cat_d.values())).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 393656 different users\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(data.cat_d)} different users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_keys = sorted(list(cat_d.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last H.valid_pct is valid set\n",
    "train_group_keys = group_keys[:int((1 - H.valid_pct) * len(group_keys))]\n",
    "valid_group_keys = group_keys[int((1 - H.valid_pct) * len(group_keys)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users: train=383814, valid=9842\n"
     ]
    }
   ],
   "source": [
    "print(f'users: train={len(train_group_keys)}, valid={len(valid_group_keys)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dict(d, keys):\n",
    "    return { (u, t): d[u][t] for u in keys for t in d[u].keys() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_cat =  split_dict(cat_d, train_group_keys)\n",
    "train_x_cont = split_dict(cont_d, train_group_keys)\n",
    "train_x_tags = split_dict(tags_d, train_group_keys)\n",
    "train_x_tagw = split_dict(tagw_d, train_group_keys)\n",
    "train_y =      split_dict(y_d, train_group_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x_cat =  split_dict(cat_d, valid_group_keys)\n",
    "valid_x_cont = split_dict(cont_d, valid_group_keys)\n",
    "valid_x_tags = split_dict(tags_d, valid_group_keys)\n",
    "valid_x_tagw = split_dict(tagw_d, valid_group_keys)\n",
    "valid_y =      split_dict(y_d, valid_group_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqs: train=507050, valid=12914\n"
     ]
    }
   ],
   "source": [
    "print(f'seqs: train={len(train_x_cat)}, valid={len(valid_x_cat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionsDataset(Dataset):\n",
    "    def __init__(self, x_cat, x_cont, x_tags, x_tagw, y, minids=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.means = np.expand_dims(meta.means, axis=0) # ready to broadcast\n",
    "        self.stds  = np.expand_dims(meta.stds , axis=0)\n",
    "        \n",
    "        self.n_inp = 5  # number of feature (x) tensors\n",
    "        \n",
    "        self.x_cat = x_cat  # SL, XF (sequence len, feature columns) \n",
    "        self.x_cont = x_cont\n",
    "        self.x_tags = x_tags      \n",
    "        self.x_tagw = x_tagw\n",
    "        self.y = y  # SL, 1\n",
    "        \n",
    "        self.keys = list(self.x_cat.keys()) # list of group keys\n",
    "        \n",
    "        if minids:\n",
    "            self.keys = self.keys[:H.bs*2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys) # H.bs * 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_id, time_slice = self.keys[idx]\n",
    "        win = range(max(0, time_slice - H.n_chunks + 1), time_slice + 1)\n",
    "        x_cat  = np.concatenate([ self.x_cat [(user_id, ts)] for ts in win ])\n",
    "        x_cont = np.concatenate([ self.x_cont[(user_id, ts)] for ts in win ])\n",
    "        x_tags = np.concatenate([ self.x_tags[(user_id, ts)] for ts in win ])\n",
    "        x_tagw = np.concatenate([ self.x_tagw[(user_id, ts)] for ts in win ])\n",
    "        y      = np.concatenate([ self.y     [(user_id, ts)] for ts in win ])\n",
    "        \n",
    "        pad = H.chunk_size * H.n_chunks - x_cat.shape[0]\n",
    "        \n",
    "        # Normalize x_cont\n",
    "        x_cont = (x_cont - self.means) / self.stds\n",
    "        x_cont[np.isnan(x_cont)] = 0\n",
    "        \n",
    "        padt = (0,pad) if H.pad == 'r' else (pad,0)\n",
    "        \n",
    "        x_mask = np.zeros(x_cat.shape[0], dtype=np.bool)\n",
    "        \n",
    "        x_mask = np.pad(x_mask, padt, constant_values=(True))\n",
    "        x_cat  = np.pad(x_cat , (padt, (0, 0)), constant_values=(0)).astype(np.int64)\n",
    "        x_cont = np.pad(x_cont, (padt, (0, 0)), constant_values=(0)).astype(np.float32)\n",
    "        x_tags = np.pad(x_tags, (padt, (0, 0)), constant_values=(0)).astype(np.int64)\n",
    "        x_tagw = np.pad(x_tagw, (padt, (0, 0)), constant_values=(0.)).astype(np.float32)\n",
    "        y      = np.pad(y,      (padt, (0, 0)), constant_values=(-1)).astype(np.int64)\n",
    "\n",
    "        return x_mask, x_cat, x_cont, x_tags, x_tagw, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = InteractionsDataset(train_x_cat, train_x_cont, train_x_tags, train_x_tagw, train_y)\n",
    "valid_ds = InteractionsDataset(valid_x_cat, valid_x_cont, valid_x_tags, valid_x_tagw, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mask, x_cat, x_cont, x_tags, x_tagw, y = train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507050"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_tagw[-47:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x_cat.shape == (H.chunk_size*H.n_chunks, len(meta.cat_names))\n",
    "assert x_cont.shape == (H.chunk_size*H.n_chunks, len(meta.cont_names))\n",
    "assert x_tags.shape == x_tagw.shape == (H.chunk_size*H.n_chunks, 6)\n",
    "assert y.shape == (H.chunk_size*H.n_chunks, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs=H.bs, shuffle=True, drop_last=True, num_workers=H.workers)\n",
    "valid_dl = DataLoader(valid_ds, bs=H.bs,                               num_workers=H.workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mask,x_cat, x_cont, x_tags, x_tagw, y = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True)\n",
    "torch.set_printoptions(precision=4, linewidth=200, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2928, -0.3581, -0.1834,  ..., -1.3834, -0.0427, -0.3680],\n",
       "        [-0.2928, -0.3581, -0.1834,  ..., -1.3829, -0.0427, -0.1182],\n",
       "        [-0.2928, -0.3581, -0.1834,  ..., -1.3826, -0.0427, -0.3395],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.4099, -0.0418,  0.9520],\n",
       "        [ 1.4168,  2.0655,  2.5181,  ..., -0.4109, -0.0428, -0.7637],\n",
       "        [ 1.4168,  2.0655,  2.5181,  ..., -0.4109, -0.0428, -0.7637]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cont[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([96, 500, 11]),\n",
       " torch.Size([96, 500, 23]),\n",
       " torch.Size([96, 500, 6]),\n",
       " torch.Size([96, 500, 6]),\n",
       " torch.Size([96, 500, 2]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cat.shape, x_cont.shape, x_tags.shape, x_tagw.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x_cat.isnan().any() == False\n",
    "assert x_cont.isnan().any() == False\n",
    "assert x_tags.isnan().any() == False\n",
    "assert x_tagw.isnan().any() == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x_cat.shape == (H.bs, H.chunk_size*H.n_chunks, len(meta.cat_names))\n",
    "assert x_cont.shape == (H.bs, H.chunk_size*H.n_chunks, len(meta.cont_names))\n",
    "assert x_tags.shape == x_tagw.shape == (H.bs, H.chunk_size*H.n_chunks, 6)\n",
    "assert y.shape == (H.bs, H.chunk_size*H.n_chunks, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(pred, targ):\n",
    "    pred = torch.softmax(pred, dim=2)\n",
    "    pred = pred[:,:,1:2] # prediction for True\n",
    "    idx = targ != -1\n",
    "    pred = pred[idx]\n",
    "    targ = targ[idx]\n",
    "    pred, targ = flatten_check(pred, targ)\n",
    "    if len(targ.unique()) == 2:\n",
    "        return roc_auc_score(targ.cpu().numpy(), pred.cpu().numpy())\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss if H.loss=='ce' else globals()[H.loss]\n",
    "loss    = loss_fn(ignore_index=-1)\n",
    "loss_nr = loss_fn(ignore_index=-1, reduction='none')\n",
    "\n",
    "def loss_func(pred, targ, shuffle=None, lam=None):\n",
    "    b, s, l = pred.shape\n",
    "    if shuffle is not None:\n",
    "        targ_shuffled = targ[shuffle].view(b*s)\n",
    "    pred = pred.view(b*s, l)\n",
    "    targ = targ.view(b*s)\n",
    "\n",
    "    if shuffle is not None:\n",
    "        l0 = loss_nr(pred, targ).view(b, s)\n",
    "        l1 = loss_nr(pred, targ_shuffled).view(b, s)\n",
    "        return torch.lerp(l0, l1, lam.view(lam.shape[0], 1)).mean()\n",
    "    else:\n",
    "        #print(targ.unique()) # CUDA assert error if any index here is bigger than dimension l (labels) of pred\n",
    "        return loss(pred, targ)\n",
    "    \n",
    "def ua_loss_func(pred, targ, shuffle=None, lam=None):\n",
    "    loss_fn = loss_func\n",
    "    l = loss_fn(pred[...,:2],targ[...,:1],shuffle,lam) \n",
    "    if H.wua and targ.shape[-1]>1: l += H.wua * loss_fn(pred[...,2:],targ[...,1:],shuffle,lam)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "_p = torch.zeros([32, 127, 6])\n",
    "_t = torch.empty ([32, 127,2]).type(torch.long)\n",
    "_t[...,0] = torch.randint(2,_t.shape[:2])\n",
    "_t[...,1] = torch.randint(4,_t.shape[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#noexport\n",
    "roc_auc(_p[...,:2], _t[...,:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6931)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#noexport\n",
    "loss_func(_p[...,:2], _t[...,:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6931)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#noexport\n",
    "ua_loss_func(_p, _t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LBMetric(Metric):\n",
    "    def __init__(self, loss_func, name):\n",
    "        self.loss_func = loss_func\n",
    "        self.nam = name\n",
    "        \n",
    "    def reset(self):\n",
    "        self.targs, self.preds = [], []\n",
    "        \n",
    "    def accumulate(self, learn):\n",
    "        self.preds.append(learn.to_detach(learn.pred[...,:2]))\n",
    "        self.targs.append(learn.to_detach(learn.y[...,:1]))\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        if len(self.preds) == 0: return\n",
    "        preds = torch.cat(self.preds)\n",
    "        targs = torch.cat(self.targs)\n",
    "        r = self.loss_func(preds, targs)\n",
    "        return r\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.nam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#noexport\n",
    "lam = Beta(0.5, 0.5).sample((10000,))\n",
    "lam = torch.stack([lam, 1-lam], 1)\n",
    "lam = lam.max(1)[0].numpy()\n",
    "_ = plt.hist(lam, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMixUp(Callback):\n",
    "    run_after,run_valid = [Normalize],False\n",
    "    def __init__(self, alpha=0.4): \n",
    "        self.distrib = Beta(tensor(alpha), tensor(alpha))\n",
    "\n",
    "    def before_batch(self):\n",
    "        lam = self.distrib.sample((self.y.size(0),)).squeeze().to(self.y.device)\n",
    "        lam = torch.stack([lam, 1-lam], 1)\n",
    "        lam = lam.max(1)[0]\n",
    "        shuffle = torch.randperm(self.y.size(0)).to(self.y.device)\n",
    "        self.learn.xb = (*self.xb, shuffle, lam)\n",
    "        self.learn.yb = (*self.yb, shuffle, lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientClipping(Callback):\n",
    "    \"Gradient clipping during training.\"\n",
    "    def __init__(self, clip:float = 0.):\n",
    "        self.clip = clip\n",
    "\n",
    "    def after_backward(self, **kwargs):\n",
    "        \"Clip the gradient before the optimizer step.\"\n",
    "        if self.clip: nn.utils.clip_grad_norm_(self.learn.model.parameters(), self.clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TutorNet(nn.Module):\n",
    "    def __init__(self, emb_szs, tag_emb_szs, emb_do, n_cont, trf_dim, trf_enc, trf_dec, trf_heads, trf_do, trf_act):\n",
    "        super().__init__()\n",
    "        self.nhead,self.trf_dim = trf_heads, trf_dim\n",
    "        \n",
    "        tag_emb_szs =(tag_emb_szs[0]+1, trf_dim)\n",
    "\n",
    "        self.embeds    = nn.ModuleList([nn.Sequential(nn.Embedding(ni+1, nf, max_norm=1.),nn.Linear(nf, trf_dim)) \n",
    "                                        for ni, nf in emb_szs])\n",
    "        self.tagembeds = nn.EmbeddingBag(*tag_emb_szs, max_norm=1., mode='sum')\n",
    "        self.conts     = nn.Linear(n_cont, trf_dim)\n",
    "            \n",
    "        self.trafo = nn.Transformer(\n",
    "            d_model = trf_dim,\n",
    "            nhead = trf_heads,\n",
    "            num_encoder_layers = trf_enc,\n",
    "            num_decoder_layers = trf_dec,\n",
    "            dim_feedforward = trf_dim*4,\n",
    "            dropout = trf_do,\n",
    "            activation = trf_act,\n",
    "        )\n",
    "\n",
    "        self.mlp = nn.Linear(trf_dim, 6)\n",
    "        \n",
    "    def forward(self, x_mask, x_cat, x_cont, x_tags, x_tagw, shuffle=None, lam=None):\n",
    "        b, sl, catf, contf, tagsf = (*x_cat.shape, x_cont.shape[2], x_tags.shape[2])\n",
    "        \n",
    "        x_cat  += 1\n",
    "        x_tags += 1\n",
    "    \n",
    "        # compute masks\n",
    "        causal_mask  = torch.triu(torch.ones(1,sl, sl,dtype=torch.bool,device=x_cat.device), diagonal=1).expand(b,-1,-1)\n",
    "        x_tci   = x_cat[...,Cats.task_container_id]\n",
    "        x_tci_s = torch.zeros_like(x_tci)\n",
    "        x_tci_s[...,1:] = x_tci[...,:-1]\n",
    "        enc_container_aware_mask =  (x_tci.unsqueeze(-1) == x_tci_s.unsqueeze(-1).permute(0,2,1)) | causal_mask\n",
    "        dec_container_aware_mask = ~(x_tci.unsqueeze(-1) == x_tci.unsqueeze(-1).permute(0,2,1))   & causal_mask\n",
    "\n",
    "        padding_mask = x_mask \n",
    "                \n",
    "        # encoder x (shifted q & a)\n",
    "        enc_cat  = torch.zeros_like(x_cat)\n",
    "        enc_cont = torch.zeros_like(x_cont)\n",
    "        enc_tags = torch.zeros_like(x_tags)\n",
    "        enc_tagw = torch.zeros_like(x_tagw)\n",
    "        \n",
    "        enc_cat[:,1:]  = x_cat[:,:-1]\n",
    "        enc_cont[:,1:] = x_cont[:,:-1]\n",
    "        enc_tags[:,1:] = x_tags[:,:-1]\n",
    "        enc_tagw[:,1:] = x_tagw[:,:-1]\n",
    "        \n",
    "        # decoder x (nonshifted q)\n",
    "        dec_cat  = x_cat\n",
    "        dec_cont = x_cont\n",
    "        dec_tags = x_tags\n",
    "        dec_tagw = x_tagw\n",
    "\n",
    "        # hide correct answer and user answered correctly from decoder\n",
    "        dec_cat[...,Cats.answered_correctly] = 0\n",
    "        dec_cat[...,Cats.user_answer] = 0\n",
    "        dec_cat[...,Cats.qhe] = 0\n",
    "        dec_cont[...,Conts.qet] = 0\n",
    "        dec_cont[...,Conts.qet_log] = 0\n",
    "        \n",
    "        # print(enc_cont.shape)\n",
    "        enc_cat  =  enc_cat.view(b * sl, catf)   # b*sl, catf\n",
    "        enc_tags = enc_tags.view(b * sl, tagsf) # b*sl, tagsf\n",
    "        enc_tagw = enc_tagw.view(b * sl, tagsf) # b*sl, tagsf\n",
    "\n",
    "        dec_cat  =  dec_cat.view(b * sl, catf)   # b*sl, catf\n",
    "        dec_tags = dec_tags.view(b * sl, tagsf) # b*sl, tagsf\n",
    "        dec_tagw = dec_tagw.view(b * sl, tagsf) # b*sl, tagsf\n",
    "        \n",
    "        # embed categorical vars\n",
    "        enc = torch.mean(torch.stack([\n",
    "            *[ e(enc_cat[:,i]) for i, e in enumerate(self.embeds) ],\n",
    "            self.tagembeds(enc_tags, per_sample_weights=enc_tagw),\n",
    "            self.conts(enc_cont).view(-1,self.trf_dim)\n",
    "        ]),dim=0)\n",
    "        \n",
    "        dec = torch.mean(torch.stack([\n",
    "            *[ e(dec_cat[:,i]) for i, e in enumerate(self.embeds) ],\n",
    "            self.tagembeds(dec_tags, per_sample_weights=dec_tagw),\n",
    "            self.conts(dec_cont).view(-1,self.trf_dim)\n",
    "        ]),dim=0)\n",
    "        \n",
    "        enc = enc.view(b, sl, self.trf_dim)           # b, sl, sum of cat, cont and tag ftrs\n",
    "        dec = dec.view(b, sl, self.trf_dim)           # b, sl, sum of cat, cont and tag ftrs\n",
    "\n",
    "        if shuffle is not None:\n",
    "            enc = torch.lerp(enc, enc[shuffle], lam.view(lam.shape[0], 1, 1))\n",
    "            dec = torch.lerp(dec, dec[shuffle], lam.view(lam.shape[0], 1, 1))\n",
    "            padding_mask = None\n",
    "            enc_container_aware_mask = dec_container_aware_mask = causal_mask | causal_mask[shuffle]\n",
    "        \n",
    "        enc = enc.permute(1, 0, 2)          # sl, b, tf (torchformer input)\n",
    "        dec = dec.permute(1, 0, 2)          # sl, b, tf\n",
    "\n",
    "        expand_nheads = lambda t: t.unsqueeze(1).expand(t.shape[0],self.nhead,-1,-1).reshape(-1,*t.shape[-2:])\n",
    "        \n",
    "        o = self.trafo(\n",
    "            enc, \n",
    "            dec, \n",
    "            src_mask = expand_nheads(enc_container_aware_mask),\n",
    "            tgt_mask = expand_nheads(dec_container_aware_mask),\n",
    "            memory_mask = expand_nheads(enc_container_aware_mask),\n",
    "            src_key_padding_mask = padding_mask,\n",
    "            tgt_key_padding_mask = padding_mask,\n",
    "            memory_key_padding_mask = padding_mask,\n",
    "        )                                   # sl, b, tf\n",
    "        o = o.permute(1, 0, 2)              # b, sl, tf\n",
    "        o = self.mlp(o)                     # b, sl, of (of=2)\n",
    "        #print(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_szs = list(zip(meta.n_emb.values(), meta.emb_dim.values()))\n",
    "tag_emb_szs = meta.tags_n_emb, meta.tags_emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TutorNet(emb_szs, tag_emb_szs, None, len(meta.cont_names), \n",
    "                 H.trf_dim, H.trf_enc, H.trf_dec, H.trf_heads, H.trf_do, H.trf_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-Fixup init\n",
    "\n",
    "1. Apply Xavier initialization for all parameters excluding input embeddings. Use Gaussian initialization $N(0,d^{-\\frac{1}{2}})$ for input embeddings where d is the embedding dimension.\n",
    "\n",
    "2. Scale $v_{d}$ and $w_{d}$ matrices in each decoder attention block, weight matrices in each decoder MLP block and input embeddings $x$ and $y$ in encoder and decoder by $(9N)^{−\\frac{1}{4}}$: [code](https://github.com/layer6ai-labs/T-Fixup/blob/f1fae213ce7b48829f81632d0c96bb039b7c450e/fairseq/modules/transformer_layer.py#L161), [code](https://github.com/layer6ai-labs/T-Fixup/blob/f1fae213ce7b48829f81632d0c96bb039b7c450e/fairseq/models/transformer.py#L378), [code](https://github.com/layer6ai-labs/T-Fixup/blob/f1fae213ce7b48829f81632d0c96bb039b7c450e/fairseq/models/transformer.py#L604)\n",
    "\n",
    "3. Scale $v_{e}$ and $w_{e}$ matrices in each encoder attention block and weight matrices in each encoder MLP block by $0.67N^{−\\frac{1}{4}}$: [code](https://github.com/layer6ai-labs/T-Fixup/blob/f1fae213ce7b48829f81632d0c96bb039b7c450e/fairseq/modules/transformer_layer.py#L36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trunc_normal_(x, mean=0., std=1.):\n",
    "    \"Truncated normal initialization (approximation)\"\n",
    "    # From https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/12\n",
    "    return x.normal_().fmod_(2).mul_(std).add_(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if H.tfixup:\n",
    "    for n,p in model.named_parameters():\n",
    "        if re.match(r'.*bias$|.*bn\\.weight$|.*norm.*\\.weight',n): continue\n",
    "        gain = 1.\n",
    "        if re.match(r'.*decoder.*',n): \n",
    "            gain = (9*H.trf_dec)**(-1./4.)\n",
    "            if re.match(f'.*in_proj_weight$',n): gain *= (2**0.5)\n",
    "        elif re.match(r'.*encoder.*',n): \n",
    "            gain = 0.67*(H.trf_enc**(-1./4.))\n",
    "            if re.match(f'.*in_proj_weight$',n): gain *= (2**0.5)\n",
    "        if re.match(r'^embeds|^tagembeds', n): \n",
    "            trunc_normal_(p.data,std=(4.5*(H.trf_enc+H.trf_dec))**(-1./4.)*H.trf_dim**(-0.5))\n",
    "        else:                                  \n",
    "            nn.init.xavier_normal_(p,gain=gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if H.tfixup:\n",
    "    class MyModelPatcher(ModelPatcher):\n",
    "        def new_child_module(self, child_module_name, child_module, patch_info): return nn.Identity()\n",
    "    mp = MyModelPatcher()\n",
    "    mp.add_pattern(r\".*norm\\d?.*\",{})\n",
    "    mp.patch_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TutorNet(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Embedding(3, 1, max_norm=1.0)\n",
       "      (1): Linear(in_features=1, out_features=512, bias=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Embedding(5, 3, max_norm=1.0)\n",
       "      (1): Linear(in_features=3, out_features=512, bias=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Embedding(9767, 274, max_norm=1.0)\n",
       "      (1): Linear(in_features=274, out_features=512, bias=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Embedding(6, 4, max_norm=1.0)\n",
       "      (1): Linear(in_features=4, out_features=512, bias=True)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Embedding(420, 47, max_norm=1.0)\n",
       "      (1): Linear(in_features=47, out_features=512, bias=True)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Embedding(9, 5, max_norm=1.0)\n",
       "      (1): Linear(in_features=5, out_features=512, bias=True)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Embedding(4, 3, max_norm=1.0)\n",
       "      (1): Linear(in_features=3, out_features=512, bias=True)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Embedding(13525, 329, max_norm=1.0)\n",
       "      (1): Linear(in_features=329, out_features=512, bias=True)\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Embedding(10002, 278, max_norm=1.0)\n",
       "      (1): Linear(in_features=278, out_features=512, bias=True)\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Embedding(6, 4, max_norm=1.0)\n",
       "      (1): Linear(in_features=4, out_features=512, bias=True)\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): Embedding(7, 4, max_norm=1.0)\n",
       "      (1): Linear(in_features=4, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (tagembeds): EmbeddingBag(190, 512, max_norm=1.0, mode=sum)\n",
       "  (conts): Linear(in_features=23, out_features=512, bias=True)\n",
       "  (trafo): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "          (norm3): Identity()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "          (norm3): Identity()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "          (norm3): Identity()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): Identity()\n",
       "          (norm2): Identity()\n",
       "          (norm3): Identity()\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): Identity()\n",
       "    )\n",
       "  )\n",
       "  (mlp): Linear(in_features=512, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#noexport\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dls.cuda()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delegates(Lamb)\n",
    "def ranger_lamb(p, lr, mom=0.95, wd=0.01, eps=1e-6, **kwargs):\n",
    "    return Lookahead(Lamb(p, lr=lr, mom=mom, wd=wd, eps=eps, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(\n",
    "    dls,\n",
    "    model,\n",
    "    loss_func=ua_loss_func,\n",
    "    opt_func=partial(globals()[H.opt],**H.opt_kwargs),\n",
    "    moms = H.moms,\n",
    "    metrics=[\n",
    "        LBMetric(loss_func, 'acc_valid_loss'),\n",
    "        LBMetric(roc_auc, 'acc_roc_auc'),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_fp16 = getattr(learn,H.fp16,None)\n",
    "if f_fp16: f_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank0_only(func, *args, **kwargs):\n",
    "    \"Execute `func` in the Rank-0 process first, then in other ranks in parallel.\"\n",
    "    if args or kwargs: func = partial(func, *args, **kwargs)\n",
    "    dummy_l = Learner(DataLoaders(device='cpu'), nn.Linear(1,1), loss_func=lambda: 0)\n",
    "    res = None\n",
    "    with dummy_l.distrib_ctx():\n",
    "        if not rank_distrib(): res = func()\n",
    "        distrib_barrier()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def load(learn:Learner,fn,with_opt=False):\n",
    "    def __inner(learn:Learner,fn,with_opt=False):\n",
    "        m_dict = torch.load(f\"{(Path(learn.model_dir) / fn)}.pth\")#['model']\n",
    "        ks = []\n",
    "        for attempts in range(2):\n",
    "            try:\n",
    "                res = learn.model.load_state_dict(m_dict,strict=False)\n",
    "                print(f\"Loaded {fn} ignoring: {' '.join(ks)} and {res}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                for k in [m[1] for m in [re.match(r\"^.*mismatch for ([\\w\\.]+):\",l) for l in str(e).split(\"\\n\")] if m is not None]:\n",
    "                    m_dict.pop(k,None)\n",
    "                    ks.append(k)\n",
    "        return learn\n",
    "    return rank0_only(__inner, learn, fn, with_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "if H.load:\n",
    "    learn.load(H.load, with_opt=False)\n",
    "    print(f\"Loaded: {H.load}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if H.clip: \n",
    "    learn.add_cb(GradientClipping(H.clip))\n",
    "    print('clip on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if H.local_rank is not None: \n",
    "    learn.to_distributed(H.local_rank)\n",
    "    print('local_rank on')\n",
    "if H.mixup: \n",
    "    learn.add_cb(MyMixUp(0.5))\n",
    "    print('mixup on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "if H.validate:\n",
    "    res = learn.validate()\n",
    "    print(f\"CV: {res[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>acc_valid_loss</th>\n",
       "      <th>acc_roc_auc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TutorNet (Input shape: ['96 x 500', '96 x 500 x 11', '96 x 500 x 23', '96 x 500 x 6', '96 x 500 x 6'])\n",
       "================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "================================================================\n",
       "Embedding            96 x 1               3          True      \n",
       "________________________________________________________________\n",
       "Linear               96 x 512             1,024      True      \n",
       "________________________________________________________________\n",
       "Embedding            96 x 3               15         True      \n",
       "________________________________________________________________\n",
       "Linear               96 x 512             2,048      True      \n",
       "________________________________________________________________\n",
       "Embedding            96 x 274             2,676,158  True      \n",
       "________________________________________________________________\n",
       "Linear               96 x 512             140,800    True      \n",
       "________________________________________________________________\n",
       "Embedding            96 x 4               24         True      \n",
       "________________________________________________________________\n",
       "Linear               96 x 512             2,560      True      \n",
       "________________________________________________________________\n",
       "Embedding            96 x 47              19,740     True      \n",
       "________________________________________________________________\n",
       "Linear               96 x 512             24,576     True      \n",
       "________________________________________________________________\n",
       "Embedding            96 x 5               45         True      \n",
       "________________________________________________________________\n",
       "Linear               96 x 512             3,072      True      \n",
       "________________________________________________________________\n",
       "Embedding            96 x 3               12         True      \n",
       "________________________________________________________________\n",
       "Linear               96 x 512             2,048      True      \n",
       "________________________________________________________________\n",
       "Embedding            96 x 329             4,449,725  True      \n",
       "________________________________________________________________\n",
       "Linear               96 x 512             168,960    True      \n",
       "________________________________________________________________\n",
       "Embedding            96 x 278             2,780,556  True      \n",
       "________________________________________________________________\n",
       "Linear               96 x 512             142,848    True      \n",
       "________________________________________________________________\n",
       "Embedding            96 x 4               24         True      \n",
       "________________________________________________________________\n",
       "Linear               96 x 512             2,560      True      \n",
       "________________________________________________________________\n",
       "Embedding            96 x 4               28         True      \n",
       "________________________________________________________________\n",
       "Linear               96 x 512             2,560      True      \n",
       "________________________________________________________________\n",
       "EmbeddingBag         96 x 512             97,280     True      \n",
       "________________________________________________________________\n",
       "Linear               96 x 500 x 512       12,288     True      \n",
       "________________________________________________________________\n",
       "Linear               96 x 1 x 2048        1,050,624  True      \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 2048        0          False     \n",
       "________________________________________________________________\n",
       "Linear               96 x 1 x 512         1,049,088  True      \n",
       "________________________________________________________________\n",
       "Identity             96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Identity             96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Linear               96 x 1 x 2048        1,050,624  True      \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 2048        0          False     \n",
       "________________________________________________________________\n",
       "Linear               96 x 1 x 512         1,049,088  True      \n",
       "________________________________________________________________\n",
       "Identity             96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Identity             96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Linear               96 x 1 x 2048        1,050,624  True      \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 2048        0          False     \n",
       "________________________________________________________________\n",
       "Linear               96 x 1 x 512         1,049,088  True      \n",
       "________________________________________________________________\n",
       "Identity             96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Identity             96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Linear               96 x 1 x 2048        1,050,624  True      \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 2048        0          False     \n",
       "________________________________________________________________\n",
       "Linear               96 x 1 x 512         1,049,088  True      \n",
       "________________________________________________________________\n",
       "Identity             96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Identity             96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Identity             96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Linear               96 x 1 x 2048        1,050,624  True      \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 2048        0          False     \n",
       "________________________________________________________________\n",
       "Linear               96 x 1 x 512         1,049,088  True      \n",
       "________________________________________________________________\n",
       "Identity             96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Identity             96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Identity             96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Linear               96 x 1 x 2048        1,050,624  True      \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 2048        0          False     \n",
       "________________________________________________________________\n",
       "Linear               96 x 1 x 512         1,049,088  True      \n",
       "________________________________________________________________\n",
       "Identity             96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Identity             96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Identity             96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Linear               96 x 1 x 2048        1,050,624  True      \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 2048        0          False     \n",
       "________________________________________________________________\n",
       "Linear               96 x 1 x 512         1,049,088  True      \n",
       "________________________________________________________________\n",
       "Identity             96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Identity             96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Identity             96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Linear               96 x 1 x 2048        1,050,624  True      \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 2048        0          False     \n",
       "________________________________________________________________\n",
       "Linear               96 x 1 x 512         1,049,088  True      \n",
       "________________________________________________________________\n",
       "Identity             96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Identity             96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Identity             96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Dropout              96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Identity             96 x 1 x 512         0          False     \n",
       "________________________________________________________________\n",
       "Linear               96 x 500 x 6         3,078      True      \n",
       "________________________________________________________________\n",
       "\n",
       "Total params: 27,329,728\n",
       "Total trainable params: 27,329,728\n",
       "Total non-trainable params: 0\n",
       "\n",
       "Optimizer used: functools.partial(<function ranger_lamb at 0x7f0e5ad5c710>)\n",
       "Loss function: <function ua_loss_func at 0x7f0e5afb5200>\n",
       "\n",
       "Model unfrozen\n",
       "\n",
       "Callbacks:\n",
       "  - ModelToHalf\n",
       "  - TrainEvalCallback\n",
       "  - Recorder\n",
       "  - ProgressCallback\n",
       "  - MixedPrecision"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#noexport\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "#learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "#learn.recorder.plot_lr_find(skip_end=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#short_cb = learn.add_cb(ShortEpochCallback(pct=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai.learner.Learner at 0x7f0e5ad30c50>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.add_cb(SaveModelCallback(monitor='acc_roc_auc', comp=np.greater, fname=f'best{H.model}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_flat_cos 30 0.003 {'pct_start': 0.5, 'div_final': 100.0}\n"
     ]
    }
   ],
   "source": [
    "print(H.fit, H.epochs, H.lr, H.fit_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>26.809057</td>\n",
       "      <td>22.274647</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>18.625198</td>\n",
       "      <td>6.343454</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>12.141135</td>\n",
       "      <td>0.762136</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8.190343</td>\n",
       "      <td>0.010251</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.852651</td>\n",
       "      <td>0.098772</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.357941</td>\n",
       "      <td>0.088523</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.321552</td>\n",
       "      <td>0.034557</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.569994</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.011533</td>\n",
       "      <td>0.006394</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.589603</td>\n",
       "      <td>0.007902</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.265800</td>\n",
       "      <td>0.008190</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.014021</td>\n",
       "      <td>0.007821</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.816269</td>\n",
       "      <td>0.007262</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.659685</td>\n",
       "      <td>0.006953</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.534984</td>\n",
       "      <td>0.006746</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.435179</td>\n",
       "      <td>0.006678</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.355001</td>\n",
       "      <td>0.006660</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.290359</td>\n",
       "      <td>0.006688</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.238117</td>\n",
       "      <td>0.006782</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.195826</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.161512</td>\n",
       "      <td>0.006786</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.133631</td>\n",
       "      <td>0.006818</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.110956</td>\n",
       "      <td>0.006797</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.092532</td>\n",
       "      <td>0.006758</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>0.006772</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.065257</td>\n",
       "      <td>0.006759</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.055279</td>\n",
       "      <td>0.006770</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.047153</td>\n",
       "      <td>0.006773</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.040523</td>\n",
       "      <td>0.006773</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.035114</td>\n",
       "      <td>0.006774</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#noexport\n",
    "syn_learn = synth_learner()\n",
    "getattr(syn_learn,H.fit)(H.epochs, H.lr,**H.fit_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#noexport\n",
    "syn_learn.recorder.plot_sched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/30 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>acc_valid_loss</th>\n",
       "      <th>acc_roc_auc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='422' class='' max='5281' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      7.99% [422/5281 03:23<39:06 0.5658]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Fitting {H.local_rank}\")\n",
    "getattr(learn,H.fit)(H.epochs, H.lr,**H.fit_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "learn.recorder.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noexport\n",
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd2227845cbcd8f6529ddbbcfe1105e000bfe4a039b6661f9778f4feee5d9853"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('riiid-acp': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
